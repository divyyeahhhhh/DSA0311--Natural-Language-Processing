{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI9h3XJ4C0BPlhN2cfIup4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyyeahhhhh/DSA0311--Natural-Language-Processing/blob/main/DSA0311(NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tWrite program demonstrates how to use regular expressions in Python to match and search for patterns in text.**"
      ],
      "metadata": {
        "id": "5Eh7cCZ0yexc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_emails(text):\n",
        "    # Define a simple regular expression for matching email addresses\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "\n",
        "    # Use re.findall to find all matches in the text\n",
        "    matches = re.findall(email_pattern, text)\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Example text containing email addresses\n",
        "sample_text = \"Contact us at info@example.com or support@company.com for assistance.\"\n",
        "\n",
        "# Find and print all email addresses in the text\n",
        "email_addresses = find_emails(sample_text)\n",
        "print(\"Email Addresses Found:\")\n",
        "print(email_addresses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF6TnXusyiGo",
        "outputId": "4769bc6d-84c6-48d8-9e9b-92798072220f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email Addresses Found:\n",
            "['info@example.com', 'support@company.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.\tImplement a basic finite state automaton that recognizes a specific language or pattern. In this example, we'll create a simple automaton to match strings ending with 'ab' using python.**"
      ],
      "metadata": {
        "id": "_JITW1KJyspk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_match(input_string):\n",
        "    # Define the finite state automaton transitions\n",
        "    transitions = {\n",
        "        0: {'a': 1, 'b': 0},\n",
        "        1: {'a': 1, 'b': 2},\n",
        "        2: {'a': 1, 'b': 0}\n",
        "    }\n",
        "    current_state = 0\n",
        "    # Process each character in the input string\n",
        "    for char in input_string:\n",
        "        if char in transitions[current_state]:\n",
        "            current_state = transitions[current_state][char]\n",
        "        else:\n",
        "            # If there is no transition for the current character, reset to the initial state\n",
        "            current_state = 0\n",
        "    # Check if the final state is reached\n",
        "    return current_state == 2\n",
        "# Test the automaton with various strings\n",
        "test_strings = [\"ab\", \"aab\", \"aaaab\", \"abc\", \"xyzab\", \"abab\", \"ba\"]\n",
        "for test_string in test_strings:\n",
        "    if is_match(test_string):\n",
        "        print(f\"'{test_string}' matches the pattern.\")\n",
        "    else:\n",
        "        print(f\"'{test_string}' does not match the pattern.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oKeY14Wy5MS",
        "outputId": "97371982-86fe-4295-a7f2-c9dfae9ec06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'ab' matches the pattern.\n",
            "'aab' matches the pattern.\n",
            "'aaaab' matches the pattern.\n",
            "'abc' does not match the pattern.\n",
            "'xyzab' matches the pattern.\n",
            "'abab' matches the pattern.\n",
            "'ba' does not match the pattern.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.\tWrite program demonstrates how to perform morphological analysis using the NLTK library in Python.**"
      ],
      "metadata": {
        "id": "dNSeYMvly_U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')  # Download the punkt tokenizer if not already downloaded\n",
        "\n",
        "def perform_morphological_analysis(text):\n",
        "    # Tokenize the input text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Create a Porter stemmer object\n",
        "    porter_stemmer = PorterStemmer()\n",
        "\n",
        "    # Perform stemming on each word\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "    return stemmed_words\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example text for morphological analysis\n",
        "    input_text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
        "\n",
        "    # Perform morphological analysis (stemming)\n",
        "    result = perform_morphological_analysis(input_text)\n",
        "\n",
        "    # Display the original and stemmed words\n",
        "    print(\"Original words:\", nltk.word_tokenize(input_text))\n",
        "    print(\"Stemmed words:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnXvEWowzz14",
        "outputId": "47a29214-ed42-4c4e-ea51-878c9511053e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs']\n",
            "Stemmed words: ['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.\tImplement a finite-state machine for morphological parsing. In this example, we'll create a simple machine to generate plural forms of English nouns using python.**"
      ],
      "metadata": {
        "id": "mx_PLZ-sz33V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, word_tokenize\n",
        "def identify_nouns(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged_words = pos_tag(words)\n",
        "    print(tagged_words)\n",
        "\n",
        "    nouns = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "\n",
        "    return nouns\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "nouns = identify_nouns(sentence)\n",
        "\n",
        "if nouns:\n",
        "    print(\"Nouns identified in the sentence:\")\n",
        "    for noun in nouns:\n",
        "        if noun[-1].lower() in {'s', 'x', 'z'} or noun[-2:].lower() in {'ch', 'sh'}:\n",
        "            print(noun+\"es\")\n",
        "        else:\n",
        "            print(noun+\"s\")\n",
        "else:\n",
        "    print(\"No nouns found in the sentence.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5TIZKD40OOr",
        "outputId": "f5030c55-5564-426a-b79f-c41391a77875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
            "Nouns identified in the sentence:\n",
            "browns\n",
            "foxes\n",
            "dogs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.\tUse the Porter Stemmer algorithm to perform word stemming on a list of words using python libraries.**"
      ],
      "metadata": {
        "id": "gIsmp4PF0gPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def perform_stemming(words):\n",
        "    # Initialize the Porter Stemmer\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    # Perform stemming for each word\n",
        "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "words_to_stem = [\"running\", \"jumps\", \"happily\", \"dogs\", \"cats\", \"better\"]\n",
        "\n",
        "stemmed_words = perform_stemming(words_to_stem)\n",
        "\n",
        "print(\"Original Words:\", words_to_stem)\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAzgtHze0mQ0",
        "outputId": "4dd36668-d000-4ed8-b87e-b3bb80e07fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'jumps', 'happily', 'dogs', 'cats', 'better']\n",
            "Stemmed Words: ['run', 'jump', 'happili', 'dog', 'cat', 'better']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.\tImplement a basic N-gram model for text generation. For example, generate text using a bigram model using python.**"
      ],
      "metadata": {
        "id": "CHj2mXnH0tFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def build_bigram_model(sentences):\n",
        "    bigram_model = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.split()\n",
        "        for i in range(len(tokens) - 1):\n",
        "            current_word = tokens[i]\n",
        "            next_word = tokens[i + 1]\n",
        "\n",
        "            if current_word in bigram_model:\n",
        "\n",
        "                bigram_model[current_word].append(next_word)\n",
        "            else:\n",
        "                bigram_model[current_word] = [next_word]\n",
        "\n",
        "    return bigram_model\n",
        "\n",
        "def generate_text(bigram_model, start_word, length=10):\n",
        "    generated_text = [start_word]\n",
        "\n",
        "    for _ in range(length - 1):\n",
        "        if start_word in bigram_model:\n",
        "            next_word = random.choice(bigram_model[start_word])\n",
        "            generated_text.append(next_word)\n",
        "            start_word = next_word\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Example list of sentences\n",
        "sentences = [\n",
        "    \"I love programming in Python.\",\n",
        "    \"Python is a versatile programming language.\",\n",
        "    \"Text generation using bigram models is interesting.\",\n",
        "    \"Natural Language Processing involves analyzing and generating text.\"\n",
        "]\n",
        "\n",
        "# Build bigram model\n",
        "bigram_model = build_bigram_model(sentences)\n",
        "print(bigram_model)\n",
        "\n",
        "# Generate text using bigram model\n",
        "generated_text = generate_text(bigram_model, start_word=\"I\", length=8)\n",
        "\n",
        "# Display the results\n",
        "print(\"Generated Text:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps3fhXbm1tD4",
        "outputId": "0c28e7e3-dad3-45fb-a7f6-637cbaa8493f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I': ['love'], 'love': ['programming'], 'programming': ['in', 'language.'], 'in': ['Python.'], 'Python': ['is'], 'is': ['a', 'interesting.'], 'a': ['versatile'], 'versatile': ['programming'], 'Text': ['generation'], 'generation': ['using'], 'using': ['bigram'], 'bigram': ['models'], 'models': ['is'], 'Natural': ['Language'], 'Language': ['Processing'], 'Processing': ['involves'], 'involves': ['analyzing'], 'analyzing': ['and'], 'and': ['generating'], 'generating': ['text.']}\n",
            "Generated Text: I love programming in Python.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.\tWrite program using the NLTK library to perform part-of-speech tagging on a text.**"
      ],
      "metadata": {
        "id": "4EIiIZfe2LvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "def perform_pos_tagging(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Perform part-of-speech tagging\n",
        "    tagged_words = pos_tag(words)\n",
        "    return tagged_words\n",
        "# Example text\n",
        "text = \"NLTK is a powerful library for natural language processing.\"\n",
        "# Perform part-of-speech tagging\n",
        "tagged_words = perform_pos_tagging(text)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Part-of-Speech Tagging Result:\", tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR61a-Di2aXX",
        "outputId": "2e679f78-ac0d-42ee-81ed-d5ffc09a7448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: NLTK is a powerful library for natural language processing.\n",
            "Part-of-Speech Tagging Result: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.\tImplement a simple stochastic part-of-speech tagging algorithm using a basic probabilistic model to assign POS tags using python.**"
      ],
      "metadata": {
        "id": "x6wEnWpp2gRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def train_unigram_model(tagged_corpus):\n",
        "    unigram_model = {}\n",
        "\n",
        "    for sentence in tagged_corpus:\n",
        "        for word, pos_tag in sentence:\n",
        "            if word in unigram_model:\n",
        "                unigram_model[word].append(pos_tag)\n",
        "            else:\n",
        "                unigram_model[word] = [pos_tag]\n",
        "\n",
        "    return unigram_model\n",
        "\n",
        "def stochastic_pos_tagging(sentence, unigram_model):\n",
        "    tagged_sentence = []\n",
        "\n",
        "    for word in sentence:\n",
        "        if word in unigram_model:\n",
        "            pos_tag = random.choice(unigram_model[word])\n",
        "        else:\n",
        "            # If word not in model, assign a default POS tag (e.g., 'NOUN')\n",
        "            pos_tag = 'NOUN'\n",
        "\n",
        "        tagged_sentence.append((word, pos_tag))\n",
        "\n",
        "    return tagged_sentence\n",
        "\n",
        "# Example tagged corpus for training\n",
        "tagged_corpus = [\n",
        "    [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN')],\n",
        "    [('Jumped', 'VERB'), ('over', 'PREP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n",
        "]\n",
        "\n",
        "# Train unigram model\n",
        "unigram_model = train_unigram_model(tagged_corpus)\n",
        "\n",
        "# Example sentence for stochastic POS tagging\n",
        "sentence_to_tag = ['The', 'lazy', 'fox', 'jumped']\n",
        "\n",
        "# Perform stochastic POS tagging\n",
        "tagged_sentence = stochastic_pos_tagging(sentence_to_tag, unigram_model)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Sentence:\", sentence_to_tag)\n",
        "print(\"Stochastic POS Tagging Result:\", tagged_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro3G-EhN2l8G",
        "outputId": "33d6ac81-54bb-4888-b40a-a48958c2de0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ['The', 'lazy', 'fox', 'jumped']\n",
            "Stochastic POS Tagging Result: [('The', 'DET'), ('lazy', 'ADJ'), ('fox', 'NOUN'), ('jumped', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.\tImplement a rule-based part-of-speech tagging system using regular expressions using python.**"
      ],
      "metadata": {
        "id": "U3Bqj_J-2yVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def rule_based_pos_tagging(sentence):\n",
        "    tagged_sentence = []\n",
        "    for word in sentence:\n",
        "        if re.match(r'\\b(?:is|am|are|was|were)\\b', word, re.IGNORECASE):\n",
        "            pos_tag = 'VERB'\n",
        "        elif re.match(r'\\b(?:the|a|an)\\b', word, re.IGNORECASE):\n",
        "            pos_tag = 'DET'\n",
        "        elif re.match(r'\\b(?:quick|brown|lazy)\\b', word, re.IGNORECASE):\n",
        "            pos_tag = 'ADJ'\n",
        "        else:\n",
        "            pos_tag = 'NOUN'\n",
        "\n",
        "        tagged_sentence.append((word, pos_tag))\n",
        "    return tagged_sentence\n",
        "# Example sentence for rule-based POS tagging\n",
        "sentence_to_tag = ['The', 'quick', 'brown', 'fox', 'is', 'lazy']\n",
        "# Perform rule-based POS tagging\n",
        "tagged_sentence = rule_based_pos_tagging(sentence_to_tag)\n",
        "# Display the results\n",
        "print(\"Original Sentence:\", sentence_to_tag)\n",
        "print(\"Rule-based POS Tagging Result:\", tagged_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMAtoapr23j5",
        "outputId": "f4ca317a-49e8-4bc1-f480-8d6665031dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ['The', 'quick', 'brown', 'fox', 'is', 'lazy']\n",
            "Rule-based POS Tagging Result: [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('is', 'VERB'), ('lazy', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Implement transformation-based tagging using a set of transformation rules, apply a simple rule to tag words using python.py**"
      ],
      "metadata": {
        "id": "NJbGWd1TiAIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_transformation_rule(word):\n",
        "    # Apply a simple transformation rule (example rule: if a word ends with 'ing', tag it as a 'VERB')\n",
        "    if word.lower().endswith('ing'):\n",
        "        return 'VERB'\n",
        "    else:\n",
        "        return 'NOUN'\n",
        "def transform_based_pos_tagging(sentence):\n",
        "    tagged_sentence = []\n",
        "\n",
        "    for word in sentence:\n",
        "        pos_tag = apply_transformation_rule(word)\n",
        "        tagged_sentence.append((word, pos_tag))\n",
        "\n",
        "    return tagged_sentence\n",
        "\n",
        "# Example sentence for transformation-based POS tagging\n",
        "sentence_to_tag = ['The', 'running', 'dog', 'is', 'jumping']\n",
        "# Perform transformation-based POS tagging\n",
        "tagged_sentence = transform_based_pos_tagging(sentence_to_tag)\n",
        "# Display the results\n",
        "print(\"Original Sentence:\", sentence_to_tag)\n",
        "print(\"Transformation-based POS Tagging Result:\", tagged_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu0jPRlXiMw9",
        "outputId": "611af6f6-ac8c-4a2e-dc5d-d7e14511718c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ['The', 'running', 'dog', 'is', 'jumping']\n",
            "Transformation-based POS Tagging Result: [('The', 'NOUN'), ('running', 'VERB'), ('dog', 'NOUN'), ('is', 'NOUN'), ('jumping', 'VERB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.Implement a simple top-down parser for context-free grammars using python very simple program**"
      ],
      "metadata": {
        "id": "7kEFDAVdiZ7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Parser:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = tokens\n",
        "        self.current_token = None\n",
        "        self.index = -1\n",
        "        self.advance()\n",
        "\n",
        "    def advance(self):\n",
        "        self.index += 1\n",
        "        if self.index < len(self.tokens):\n",
        "            self.current_token = self.tokens[self.index]\n",
        "        else:\n",
        "            self.current_token = None\n",
        "\n",
        "    def parse(self):\n",
        "        return self.expr()\n",
        "\n",
        "    def expr(self):\n",
        "        result = self.term()\n",
        "\n",
        "        while self.current_token in ('+', '-'):\n",
        "            operator = self.current_token\n",
        "            self.advance()\n",
        "            right = self.term()\n",
        "            if operator == '+':\n",
        "                result += right\n",
        "            elif operator == '-':\n",
        "                result -= right\n",
        "\n",
        "        return result\n",
        "\n",
        "    def term(self):\n",
        "        result = self.factor()\n",
        "\n",
        "        while self.current_token in ('*', '/'):\n",
        "            operator = self.current_token\n",
        "            self.advance()\n",
        "            right = self.factor()\n",
        "            if operator == '*':\n",
        "                result *= right\n",
        "            elif operator == '/':\n",
        "                if right == 0:\n",
        "                    raise ZeroDivisionError(\"Division by zero\")\n",
        "                result /= right\n",
        "\n",
        "        return result\n",
        "\n",
        "    def factor(self):\n",
        "        token = self.current_token\n",
        "        self.advance()\n",
        "\n",
        "        if token == '(':\n",
        "            result = self.expr()\n",
        "            if self.current_token != ')':\n",
        "                raise SyntaxError(\"Expected closing parenthesis\")\n",
        "            self.advance()\n",
        "            return result\n",
        "        elif token.isdigit():\n",
        "            return int(token)\n",
        "        else:\n",
        "            raise SyntaxError(\"Invalid syntax\")\n",
        "\n",
        "\n",
        "def parse_input(input_string):\n",
        "    tokens = input_string.replace(' ', '').replace('\\t', '').split(',')\n",
        "    parser = Parser(tokens)\n",
        "    return parser.parse()\n",
        "\n",
        "input_string = '5, *, (, 3, +, 7,), +, 10'\n",
        "result = parse_input(input_string)\n",
        "print(f\"Result: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlfzQSKVifGJ",
        "outputId": "3684915d-edbf-4ad5-c3c8-e97a6f5982c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.Implement an Earley parser for context-free grammars using a simple python program**"
      ],
      "metadata": {
        "id": "MQxlvnOEilB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarleyItem:\n",
        "    def __init__(self, production, dot_position, start_column):\n",
        "        self.production = production\n",
        "        self.dot_position = dot_position\n",
        "        self.start_column = start_column\n",
        "\n",
        "def predict(grammar, column, item):\n",
        "    non_terminal = item.production[item.dot_position]\n",
        "    for rule in grammar.get(non_terminal, []):\n",
        "        new_item = EarleyItem(rule, 0, column)\n",
        "        if new_item not in column:\n",
        "            column.append(new_item)\n",
        "\n",
        "def scan(tokens, column, item):\n",
        "    if item.dot_position < len(tokens) and \\\n",
        "       item.production[item.dot_position] == tokens[column]:\n",
        "        new_item = EarleyItem(item.production, item.dot_position + 1, item.start_column)\n",
        "        if new_item not in column:\n",
        "            column.append(new_item)\n",
        "\n",
        "def complete(chart, column, item):\n",
        "    for entry in chart[item.start_column]:\n",
        "        if entry.dot_position < len(entry.production) and \\\n",
        "           entry.production[entry.dot_position] == item.production and \\\n",
        "           EarleyItem(entry.production, entry.dot_position + 1, entry.start_column) not in chart[column]:\n",
        "            chart[column].append(EarleyItem(entry.production, entry.dot_position + 1, entry.start_column))\n",
        "\n",
        "def earley_parse(tokens, grammar):\n",
        "    chart = {i: [] for i in range(len(tokens) + 1)}\n",
        "    start_rule = list(grammar.keys())[0]\n",
        "    chart[0].append(EarleyItem(start_rule, 0, 0))\n",
        "\n",
        "    for column in range(len(tokens) + 1):\n",
        "        for item in chart[column]:\n",
        "            if item.dot_position < len(item.production) and \\\n",
        "               isinstance(item.production[item.dot_position], str):\n",
        "                scan(tokens, column, item)\n",
        "            elif item.dot_position < len(item.production):\n",
        "                predict(grammar, chart[column], item)\n",
        "            else:\n",
        "                complete(chart, column, item)\n",
        "\n",
        "    for item in chart[len(tokens)]:\n",
        "        if item.production == [start_rule] and item.dot_position == 1 and item.start_column == 0:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Example usage:\n",
        "example_grammar = {\n",
        "    'S': [['NP', 'VP']],\n",
        "    'NP': [['Det', 'N']],\n",
        "    'VP': [['V', 'NP']],\n",
        "    'Det': ['the', 'a'],\n",
        "    'N': ['cat', 'dog'],\n",
        "    'V': ['chased', 'ate']\n",
        "}\n",
        "\n",
        "example_tokens = ['the', 'dog', 'chased', 'a', 'cat']\n",
        "\n",
        "result = earley_parse(example_tokens, example_grammar)\n",
        "print(\"Parsing successful:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpFljnRuip-K",
        "outputId": "2ce2dfa6-ff50-4fbb-a6bc-a6d2a743dd96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing successful: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.Generate a parse tree for a given sentence using a context-free grammar using python program.**"
      ],
      "metadata": {
        "id": "t721VAAQiuwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import CFG\n",
        "\n",
        "# Define a context-free grammar\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> 'John'\n",
        "    VP -> V NP\n",
        "    V -> 'likes'\n",
        "    NP -> 'pizza'\n",
        "\"\"\")\n",
        "\n",
        "# Create a parser based on the grammar\n",
        "parser = nltk.ChartParser(grammar)\n",
        "\n",
        "# Given sentence\n",
        "sentence = \"John likes pizza\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = sentence.split()\n",
        "\n",
        "# Generate and print parse trees\n",
        "for tree in parser.parse(tokens):\n",
        "    tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiy-hBini0_4",
        "outputId": "8876476a-16be-41d1-8e93-bc0b3b4eba17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       S            \n",
            "  _____|____         \n",
            " |          VP      \n",
            " |      ____|____    \n",
            " NP    V         NP \n",
            " |     |         |   \n",
            "John likes     pizza\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14.Create a program in python to check for agreement in sentences based on a context-free grammar's rules**"
      ],
      "metadata": {
        "id": "Ht5cSoE9jVVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AgreementChecker:\n",
        "    def __init__(self, grammar):\n",
        "        self.grammar = grammar\n",
        "\n",
        "    def check_subject_verb_agreement(self, subject, verb):\n",
        "        rules = self.grammar.get(\"S-V Agreement\", [])\n",
        "        for rule in rules:\n",
        "            if rule[0] == subject and rule[1] == verb:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "grammar = {\n",
        "    \"S-V Agreement\": [\n",
        "        [\"dog\", \"barks\"],\n",
        "        [\"dogs\", \"bark\"],\n",
        "        [\"cat\", \"meows\"],\n",
        "        [\"cats\", \"meow\"]\n",
        "\n",
        "    ]\n",
        "}\n",
        "\n",
        "agreement_checker = AgreementChecker(grammar)\n",
        "\n",
        "subject = \"dog\"\n",
        "verb = \"barks\"\n",
        "is_agree = agreement_checker.check_subject_verb_agreement(subject, verb)\n",
        "\n",
        "if is_agree:\n",
        "    print(f\"The subject '{subject}' and verb '{verb}' agree.\")\n",
        "else:\n",
        "    print(f\"The subject '{subject}' and verb '{verb}' do not agree.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbkwtLyHjaC2",
        "outputId": "acdaaa0a-d576-43eb-b869-525d4a0491b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subject 'dog' and verb 'barks' agree.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. Implement probabilistic context-free grammar parsing for a sentence using python**"
      ],
      "metadata": {
        "id": "dgM7pjIXjqgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "grammar = nltk.PCFG.fromstring(\"\"\"\n",
        "    S -> NP VP [1.0]\n",
        "    VP -> V NP [0.7] | VP PP [0.3]\n",
        "    PP -> P NP [1.0]\n",
        "    V -> \"saw\" [1.0]\n",
        "    P -> \"with\" [1.0]\n",
        "    NP -> N [0.4] | Det N [0.3] | NP PP [0.3]\n",
        "    N -> \"John\" [0.4] | \"Mary\" [0.4] | \"telescope\" [0.2]\n",
        "    Det -> \"a\" [1.0]\n",
        "\"\"\")\n",
        "parser = nltk.ViterbiParser(grammar)\n",
        "\n",
        "sentence = \"John saw Mary with a telescope\".split()\n",
        "\n",
        "trees = list(parser.parse(sentence))\n",
        "\n",
        "trees.sort(key=lambda tree: -tree.prob())\n",
        "\n",
        "for tree in trees:\n",
        "    print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0xkWQWCjvvv",
        "outputId": "54f2796b-5e05-49d1-eb9f-81eeda3ea15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP (N John))\n",
            "  (VP\n",
            "    (VP (V saw) (NP (N Mary)))\n",
            "    (PP (P with) (NP (Det a) (N telescope))))) (p=0.00032256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16.Implement a Python program using the SpaCy library to perform Named Entity Recognition (NER) on a given text?**"
      ],
      "metadata": {
        "id": "cccsSjO-jzz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spacy download en_core_web_sm\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Type: {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIHs-L02j7JX",
        "outputId": "1fdceaf6-5de2-47f3-eeb6-ceac29784e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Apple, Type: ORG\n",
            "Entity: U.K., Type: GPE\n",
            "Entity: $1 billion, Type: MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17.Write program demonstrates how to access WordNet, a lexical database, to retrieve synsets and explore word meanings in python**"
      ],
      "metadata": {
        "id": "OXxjDIRukNuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download WordNet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_synsets(word):\n",
        "    synsets = wordnet.synsets(word)\n",
        "    return synsets\n",
        "\n",
        "def print_synset_info(synset):\n",
        "    print(f\"Synset: {synset.name()}\")\n",
        "    print(f\"POS (Part of Speech): {synset.pos()}\")\n",
        "    print(f\"Definition: {synset.definition()}\")\n",
        "    print(f\"Examples: {synset.examples()}\")\n",
        "    print()\n",
        "\n",
        "def explore_word_meanings(word):\n",
        "    synsets = get_synsets(word)\n",
        "\n",
        "    if not synsets:\n",
        "        print(f\"No synsets found for the word '{word}'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Synsets for the word '{word}':\")\n",
        "    for synset in synsets:\n",
        "        print_synset_info(synset)\n",
        "\n",
        "    # Explore hypernyms (more abstract terms)\n",
        "    hypernyms = synsets[0].hypernyms()\n",
        "    if hypernyms:\n",
        "        print(f\"Hypernyms of '{word}':\")\n",
        "        for hypernym in hypernyms:\n",
        "            print_synset_info(hypernym)\n",
        "\n",
        "    # Explore hyponyms (more specific terms)\n",
        "    hyponyms = synsets[0].hyponyms()\n",
        "    if hyponyms:\n",
        "        print(f\"Hyponyms of '{word}':\")\n",
        "        for hyponym in hyponyms:\n",
        "            print_synset_info(hyponym)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace 'example' with the word you want to explore\n",
        "    word_to_explore = 'example'\n",
        "    explore_word_meanings(word_to_explore)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLGiNlQ7kY4V",
        "outputId": "1ed622bf-d842-4765-d957-aef213654c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synsets for the word 'example':\n",
            "Synset: example.n.01\n",
            "POS (Part of Speech): n\n",
            "Definition: an item of information that is typical of a class or group\n",
            "Examples: ['this patient provides a typical example of the syndrome', 'there is an example on page 10']\n",
            "\n",
            "Synset: model.n.07\n",
            "POS (Part of Speech): n\n",
            "Definition: a representative form or pattern\n",
            "Examples: ['I profited from his example']\n",
            "\n",
            "Synset: exemplar.n.01\n",
            "POS (Part of Speech): n\n",
            "Definition: something to be imitated\n",
            "Examples: ['an exemplar of success', 'a model of clarity', 'he is the very model of a modern major general']\n",
            "\n",
            "Synset: example.n.04\n",
            "POS (Part of Speech): n\n",
            "Definition: punishment intended as a warning to others\n",
            "Examples: ['they decided to make an example of him']\n",
            "\n",
            "Synset: case.n.01\n",
            "POS (Part of Speech): n\n",
            "Definition: an occurrence of something\n",
            "Examples: ['it was a case of bad judgment', 'another instance occurred yesterday', 'but there is always the famous example of the Smiths']\n",
            "\n",
            "Synset: exercise.n.04\n",
            "POS (Part of Speech): n\n",
            "Definition: a task performed or problem solved in order to develop skill or understanding\n",
            "Examples: ['you must work the examples at the end of each chapter in the textbook']\n",
            "\n",
            "Hypernyms of 'example':\n",
            "Synset: information.n.02\n",
            "POS (Part of Speech): n\n",
            "Definition: knowledge acquired through study or experience or instruction\n",
            "Examples: []\n",
            "\n",
            "Hyponyms of 'example':\n",
            "Synset: apology.n.03\n",
            "POS (Part of Speech): n\n",
            "Definition: a poor example\n",
            "Examples: ['it was an apology for a meal', 'a poor excuse for an automobile']\n",
            "\n",
            "Synset: exception.n.02\n",
            "POS (Part of Speech): n\n",
            "Definition: an instance that does not conform to a rule or generalization\n",
            "Examples: ['all her children were brilliant; the only exception was her last child', 'an exception tests the rule']\n",
            "\n",
            "Synset: precedent.n.01\n",
            "POS (Part of Speech): n\n",
            "Definition: an example that is used to justify similar occurrences at a later time\n",
            "Examples: []\n",
            "\n",
            "Synset: quintessence.n.03\n",
            "POS (Part of Speech): n\n",
            "Definition: the most typical example or representative of a type\n",
            "Examples: []\n",
            "\n",
            "Synset: sample.n.01\n",
            "POS (Part of Speech): n\n",
            "Definition: a small part of something intended as representative of the whole\n",
            "Examples: []\n",
            "\n",
            "Synset: specimen.n.01\n",
            "POS (Part of Speech): n\n",
            "Definition: an example regarded as typical of its class\n",
            "Examples: []\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18.Implement a simple FOPC parser for basic logical expressions using python program**"
      ],
      "metadata": {
        "id": "xvw9UW6Pk2bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyparsing import Word, alphas, alphanums, Forward, infixNotation, opAssoc\n",
        "\n",
        "identifier = Word(alphas, alphanums+\"_\")\n",
        "\n",
        "and_op = \"AND\"\n",
        "or_op = \"OR\"\n",
        "implies_op = \"IMPLIES\"\n",
        "not_op = \"NOT\"\n",
        "\n",
        "expr = Forward()\n",
        "atom = identifier | \"(\" + expr + \")\"\n",
        "term = infixNotation(atom, [\n",
        "    (not_op, 1, opAssoc.RIGHT),\n",
        "    (and_op, 2, opAssoc.LEFT),\n",
        "    (or_op, 2, opAssoc.LEFT),\n",
        "    (implies_op, 2, opAssoc.RIGHT),\n",
        "])\n",
        "\n",
        "def parse_expression(expression):\n",
        "    return term.parseString(expression, parseAll=True)\n",
        "\n",
        "logical_expression = \"P AND (Q OR NOT R) IMPLIES S\"\n",
        "parsed_expr = parse_expression(logical_expression)\n",
        "print(parsed_expr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InSuVhm7k6qQ",
        "outputId": "cb43a8da-7f88-449c-c5ee-4b7edb069f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['P', 'AND', ['Q', 'OR', ['NOT', 'R']]], 'IMPLIES', 'S']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19.Create a program for word sense disambiguation using the Lesk algorithm using python**"
      ],
      "metadata": {
        "id": "oFWQQc1Hk-jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download WordNet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def perform_lesk_algorithm(sentence, target_word):\n",
        "    sense = lesk(word_tokenize(sentence), target_word)\n",
        "\n",
        "    if sense:\n",
        "        print(f\"Target word: {target_word}\")\n",
        "        print(f\"Selected sense: {sense.name()}\")\n",
        "        print(f\"Definition: {sense.definition()}\")\n",
        "        print(f\"Examples: {sense.examples()}\")\n",
        "    else:\n",
        "        print(f\"No sense found for the word '{target_word}' in the given context.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_sentence = \"I went to the bank to deposit some money.\"\n",
        "    target_word = \"bank\"\n",
        "\n",
        "    perform_lesk_algorithm(input_sentence, target_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT8cO3bPlblQ",
        "outputId": "39063cfe-5c60-4ab0-c436-24ff4a714275"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target word: bank\n",
            "Selected sense: savings_bank.n.02\n",
            "Definition: a container (usually with a slot in the top) for keeping money at home\n",
            "Examples: ['the coin bank was empty']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20.Implement a basic information retrieval system using TF-IDF (Term Frequency-Inverse Document Frequency) for document ranking using python**"
      ],
      "metadata": {
        "id": "5cMjkqnJvisz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "query = \"this is the second document\"\n",
        "\n",
        "query_vector = vectorizer.transform([query])\n",
        "\n",
        "cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "\n",
        "document_scores = [(score, doc) for score, doc in zip(cosine_similarities, documents)]\n",
        "sorted_documents = sorted(document_scores, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "print(\"Ranked documents based on TF-IDF similarity to the query:\")\n",
        "for i, (score, doc) in enumerate(sorted_documents, start=1):\n",
        "    print(f\"Rank {i}: Similarity Score: {score:.4f}, Document: '{doc}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oFCS9mqv2Ag",
        "outputId": "d4a76810-cde1-46b6-d043-27aff604cc12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked documents based on TF-IDF similarity to the query:\n",
            "Rank 1: Similarity Score: 0.9505, Document: 'This document is the second document.'\n",
            "Rank 2: Similarity Score: 0.6042, Document: 'This is the first document.'\n",
            "Rank 3: Similarity Score: 0.6042, Document: 'Is this the first document?'\n",
            "Rank 4: Similarity Score: 0.2804, Document: 'And this is the third one.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21.Create a python program that performs syntax-driven semantic analysis by extracting noun phrases and their meanings from a sentence**"
      ],
      "metadata": {
        "id": "_VDqP26cv6jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model from spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def syntax_semantic_analysis(sentence):\n",
        "    # Process the input sentence using spaCy\n",
        "    doc = nlp(sentence)\n",
        "    # Extract noun phrases and their meanings\n",
        "    noun_phrases_and_meanings = []\n",
        "    for chunk in doc.noun_chunks:\n",
        "        # Get the head (main word) of the noun phrase\n",
        "        head_word = chunk.root.text\n",
        "\n",
        "        # Extract the meaning (definition) of the head word\n",
        "        head_meaning = get_word_meaning(head_word)\n",
        "\n",
        "        # Store the noun phrase and its meaning\n",
        "        noun_phrases_and_meanings.append({\n",
        "            'noun_phrase': chunk.text,\n",
        "            'meaning': head_meaning\n",
        "        })\n",
        "\n",
        "    return noun_phrases_and_meanings\n",
        "def get_word_meaning(word):\n",
        "    # Placeholder function to get the meaning of a word\n",
        "    # In a real-world scenario, you would use a lexical database or an API\n",
        "    # For simplicity, we'll return a dummy meaning here\n",
        "    return f\"Dummy meaning for {word}\"\n",
        "\n",
        "# Example usage\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "results = syntax_semantic_analysis(sentence)\n",
        "\n",
        "# Display the results\n",
        "for result in results:\n",
        "    print(f\"Noun Phrase: {result['noun_phrase']}, Meaning: {result['meaning']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUtFzGcUv_0f",
        "outputId": "55c90704-692a-4647-9017-05bb7cf00d29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Phrase: The quick brown fox, Meaning: Dummy meaning for fox\n",
            "Noun Phrase: the lazy dog, Meaning: Dummy meaning for dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22.Create a python program that performs reference resolution within a text.**"
      ],
      "metadata": {
        "id": "CVvghAATwGc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def resolve_references(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
        "\n",
        "    resolved_text = []\n",
        "    pronouns = set(['he', 'him', 'his', 'she', 'her', 'it', 'they', 'them', 'their'])\n",
        "\n",
        "    for tagged_sentence in tagged_sentences:\n",
        "        resolved_sentence = []\n",
        "        for word, pos in tagged_sentence:\n",
        "            if word.lower() in pronouns and len(resolved_sentence) > 0:\n",
        "                antecedent = resolved_sentence[-1]\n",
        "                resolved_sentence.append(f'({word} -> {antecedent})')\n",
        "            else:\n",
        "                resolved_sentence.append(word)\n",
        "        resolved_text.append(' '.join(resolved_sentence))\n",
        "\n",
        "    return ' '.join(resolved_text)\n",
        "\n",
        "text = \"John went to the market and He bought some fruits.\"\n",
        "\n",
        "resolved_text = resolve_references(text)\n",
        "print(resolved_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti1YYCzkwM1k",
        "outputId": "b0a51878-c873-4cd4-f65e-c05d12cb5fdf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John went to the market and (He -> and) bought some fruits .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23 Develop a python program that evaluates the coherence of a given text**"
      ],
      "metadata": {
        "id": "KtQZl824wVm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "\n",
        "def calculate_coherence(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    coherence_markers = ['however', 'therefore', 'consequently', 'nevertheless', 'furthermore', 'meanwhile', 'although', 'while', 'yet', 'moreover']\n",
        "\n",
        "    total_markers = 0\n",
        "    for sentence in sentences:\n",
        "        tokenized_sentence = nltk.word_tokenize(sentence.lower())\n",
        "        for marker in coherence_markers:\n",
        "            if marker in tokenized_sentence:\n",
        "                total_markers += 1\n",
        "\n",
        "    return total_markers\n",
        "\n",
        "text = \"The weather was terrible. However, they decided to go for a picnic. Therefore, they packed their bags and left.\"\n",
        "\n",
        "coherence_score = calculate_coherence(text)\n",
        "print(f\"Coherence Score: {coherence_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fciGvuGGxkPj",
        "outputId": "1b5d75e1-c78c-4508-953a-704906b1a596"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24.Create a python program that recognizes dialog acts in a given dialog or conversation.**"
      ],
      "metadata": {
        "id": "nKLdJvMCxpcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def recognize_dialog_acts(text):\n",
        "\n",
        "    statements = re.compile(r'^[^?!.]*[.?!]$')\n",
        "    questions = re.compile(r'.*\\?$')\n",
        "    greetings = re.compile(r'(hello|hi|hey).*', re.IGNORECASE)\n",
        "    requests = re.compile(r'(please|can you|could you).*', re.IGNORECASE)\n",
        "\n",
        "    utterances = re.split(r'[.?!]', text)\n",
        "\n",
        "    dialog_acts = []\n",
        "    for utterance in utterances:\n",
        "        utterance = utterance.strip()\n",
        "        if re.match(statements, utterance):\n",
        "            dialog_acts.append((utterance, 'Statement'))\n",
        "        elif re.match(questions, utterance):\n",
        "            dialog_acts.append((utterance, 'Question'))\n",
        "        elif re.match(greetings, utterance):\n",
        "            dialog_acts.append((utterance, 'Greeting'))\n",
        "        elif re.match(requests, utterance):\n",
        "            dialog_acts.append((utterance, 'Request'))\n",
        "        else:\n",
        "            dialog_acts.append((utterance, 'Other'))\n",
        "\n",
        "    return dialog_acts\n",
        "\n",
        "conversation = \"Hi! How are you? I'm fine, thank you. Can you pass the salt, please?\"\n",
        "\n",
        "recognized_acts = recognize_dialog_acts(conversation)\n",
        "for utterance, act_type in recognized_acts:\n",
        "    print(f\"Utterance: '{utterance}', Dialog Act: {act_type}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUopfVqBxxF3",
        "outputId": "ea6d3bee-3ce4-428b-942a-9c2b8ba5524e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utterance: 'Hi', Dialog Act: Greeting\n",
            "Utterance: 'How are you', Dialog Act: Other\n",
            "Utterance: 'I'm fine, thank you', Dialog Act: Other\n",
            "Utterance: 'Can you pass the salt, please', Dialog Act: Request\n",
            "Utterance: '', Dialog Act: Other\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25.Utilize the GPT-3 model to generate text based on a given prompt. Make sure to install the OpenAI GPT-3 library in python implementation**"
      ],
      "metadata": {
        "id": "HE1myNnEx02V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**26. Implement a machine translation program using the Hugging Face Transformers library,  translate English text to French using python.**"
      ],
      "metadata": {
        "id": "LgXs8yzXzVH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "def translate_to_french(english_sentence, model, tokenizer):\n",
        "    input_text = f\"translate English to French: {english_sentence}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
        "\n",
        "    # Generate translation\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(input_ids)\n",
        "    french_translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return french_translation\n",
        "\n",
        "# Load the model and tokenizer outside the function\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Example usage\n",
        "english_sentence = \"The cat is on the mat.\"\n",
        "french_translation = translate_to_french(english_sentence, model, tokenizer)\n",
        "print(f\"English: {english_sentence}\")\n",
        "print(f\"French: {french_translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rOSf5t-zXdZ",
        "outputId": "ac1dc359-17d4-4ead-c642-11f2364e32da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: The cat is on the mat.\n",
            "French: traduire l'anglais en français : Le chat est sur le tapis.\n"
          ]
        }
      ]
    }
  ]
}